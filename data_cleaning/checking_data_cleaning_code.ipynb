{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00082097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Question 1\n",
    "\n",
    "# Function to parse dates\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        # Use dateutil parser with dayfirst=True for mixed formats\n",
    "        date_time = parser.parse(date_str, dayfirst=True)\n",
    "        return date_time.date()\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply parsing function\n",
    "# df['clean_date'] = df['order_date'].apply(parse_date)\n",
    "\n",
    "# Convert to standardized format 'YYYY-MM-DD'\n",
    "# df['clean_date'] = pd.to_datetime(df['clean_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# If desired, replace NaT formatted strings with None or leave as-is\n",
    "# df.loc[df['clean_date'] == 'NaT', 'clean_date'] = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 2\n",
    "\n",
    "def clean_price(value):\n",
    "    if isinstance(value, (int, float)):  # Already numeric\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        # Remove currency symbols and commas, keep digits and dot\n",
    "        cleaned = re.sub(r'[^\\d.]', '', value)\n",
    "        try:\n",
    "            return float(cleaned) if '.' in cleaned else int(cleaned)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None # For other types (e.g., None)\n",
    "\n",
    "# Example usage\n",
    "# df['original_price_inr_cleaned'] = df['original_price_inr'].apply(clean_price)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 3\n",
    "\n",
    "def round_up_scale(value):\n",
    "    \"\"\"Round value up to nearest 5 or 10, whichever is appropriate.\"\"\"\n",
    "    if value <= 5:\n",
    "        return 5\n",
    "    elif value <= 10:\n",
    "        return 10\n",
    "    else:\n",
    "        # For values above 10, round up to nearest multiple of 5\n",
    "        return int(math.ceil(value / 5.0)) * 5\n",
    "\n",
    "def standardize_rating(rating):\n",
    "    if rating is None:\n",
    "        return np.nan\n",
    "\n",
    "    rating_str = str(rating).strip().lower()\n",
    "\n",
    "    # Fraction parsing\n",
    "    if '/' in rating_str:\n",
    "        try:\n",
    "            numerator, denominator = rating_str.split('/')\n",
    "            numerator = float(numerator)\n",
    "            denominator = float(denominator)\n",
    "            if denominator == 0:\n",
    "                return np.nan\n",
    "            normalized = (numerator / denominator) * 5\n",
    "            if 1.0 <= normalized <= 5.0:\n",
    "                return round(normalized, 2)\n",
    "            else:\n",
    "                return np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    # Extract numeric value\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', rating_str)\n",
    "    if match:\n",
    "        try:\n",
    "            value = float(match.group(1))\n",
    "\n",
    "            scale = round_up_scale(value)\n",
    "            normalized = (value / scale) * 5\n",
    "\n",
    "            # Clamp between 1 and 5\n",
    "            normalized = max(1.0, min(normalized, 5.0))\n",
    "            return round(normalized, 2)\n",
    "\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# df[\"customer_rating_cleaned\"] = df[\"customer_rating\"].apply(standardize_rating)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 4\n",
    "\n",
    "# Sample city names (including variants and misspellings)\n",
    "# cities = ['Bangalore', 'bengaluru', 'Mumbai', 'Bombay', 'delhi', 'New Delhi', 'mumbay', 'banaglore', 'DELHI']\n",
    "\n",
    "# df = pd.DataFrame({'customer_city': cities})\n",
    "\n",
    "# Initialize Nominatim geocoder with a user_agent\n",
    "geolocator = Nominatim(user_agent=\"osm_city_standardizer\")\n",
    "\n",
    "def geocode_city_osm(city):\n",
    "    try:\n",
    "        # Limit search to India for better accuracy\n",
    "        location = geolocator.geocode(f\"{city}, India\", language='en', exactly_one=True, addressdetails=True)\n",
    "        if location and 'address' in location.raw:\n",
    "            address = location.raw['address']\n",
    "            # Extract city or town from the address details\n",
    "            city_name = address.get('city') or address.get('town') or address.get('village')\n",
    "            \n",
    "            # Sometimes the city might be under 'state_district' or 'county' if city not found\n",
    "            if not city_name:\n",
    "                city_name = address.get('state_district') or address.get('county')\n",
    "            \n",
    "            if city_name:\n",
    "                return city_name\n",
    "            else:\n",
    "                # Fallback: use first part of display_name\n",
    "                return location.address.split(',')[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding '{city}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 1: Get unique city names\n",
    "unique_cities = df['customer_city'].unique()\n",
    "# print(f\"the len of unique cities: {len(unique_cities)}\")\n",
    "\n",
    "# Step 2: Build a mapping dictionary\n",
    "city_mapping = {}\n",
    "\n",
    "for city in unique_cities:\n",
    "    print(f\"input city is: {city}\")\n",
    "    standardized_city = geocode_city_osm(city)\n",
    "    print(f\"output city is: {standardized_city}\")\n",
    "    city_mapping[city] = standardized_city\n",
    "    time.sleep(1)  # Pause to respect Nominatim's usage policy (1 sec per request)\n",
    "\n",
    "# Step 3: Map standardized values back to the original DataFrame\n",
    "# df['customer_city_standardized'] = df['customer_city'].map(city_mapping)\n",
    "\n",
    "# to handle\n",
    "\"\"\"\n",
    "input city is: mumba\n",
    "output city is: Navi Mumbai\n",
    "input city is: chenai\n",
    "output city is: None\n",
    "input city is: Delhi NCR\n",
    "output city is: Gurugram\n",
    "\"\"\"\n",
    "\n",
    "## method 2\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"your_file.csv\")  # Replace with your file name\n",
    "input_cities = df[\"input_city\"].astype(str)\n",
    "\n",
    "# Assume your true city names list is available\n",
    "true_cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"]  # Replace with your actual list\n",
    "\n",
    "# Define a function to find the closest match\n",
    "def get_best_match(city, choices, threshold=80):\n",
    "    match, score, _ = process.extractOne(city, choices, scorer=fuzz.WRatio)\n",
    "    return match if score >= threshold else city  # fallback to original if no good match\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df[\"clean_city\"] = input_cities.apply(lambda x: get_best_match(x, true_cities))\n",
    "\n",
    "# Save the corrected file\n",
    "df.to_csv(\"corrected_cities.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 5\n",
    "\n",
    "# Define a function to normalize boolean-like values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 6\n",
    "\n",
    "df['Category_Standardized'] = 'Electronics'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 7\n",
    "\n",
    "def clean_day(value):\n",
    "    value = value.strip().lower()\n",
    "\n",
    "    # Special case: 'same day'\n",
    "    if value == \"same day\":\n",
    "        return 0\n",
    "\n",
    "    # Check for two numbers separated by a dash (e.g., \"1-2 days\")\n",
    "    range_match = re.findall(r\"\\d+(?:\\.\\d+)?\", value)\n",
    "    if '-' in value:\n",
    "        if len(range_match) == 2:\n",
    "            try:\n",
    "                return max(float(range_match[0]), float(range_match[1]))\n",
    "            except:\n",
    "                return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    # Check if it's a single valid number (possibly with 'days' word)\n",
    "    if len(range_match) == 1:\n",
    "        try:\n",
    "            num = float(range_match[0])\n",
    "            if num < 0:\n",
    "                return np.nan\n",
    "            return num\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    # If none of the above matched, it's invalid\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 8\n",
    "\n",
    "# Step 1: Find groups with potential duplicates\n",
    "grouped = df.groupby(['customer_id', 'product_id', 'order_date', 'final_amount_inr'])\n",
    "\n",
    "def analyze_group(group):\n",
    "    unique_tx_ids = group['transaction_id'].nunique()\n",
    "    total_records = len(group)\n",
    "    \n",
    "    if unique_tx_ids == total_records:\n",
    "        # All transactions have unique IDs -> likely genuine bulk order\n",
    "        return 'genuine_bulk_order'\n",
    "    elif unique_tx_ids < total_records:\n",
    "        # Some transaction_ids repeat -> data error duplicates\n",
    "        return 'data_error_duplicates'\n",
    "    else:\n",
    "        return 'ambiguous'\n",
    "\n",
    "# Apply the function to each group and create a Series with the result repeated for each row in the group\n",
    "duplicate_types = grouped.apply(analyze_group).reset_index(name='duplicate_type')\n",
    "\n",
    "# Map the group keys back to the original dataframe by merging or joining\n",
    "df = df.merge(duplicate_types.reset_index(), on=['customer_id', 'product_id', 'order_date', 'final_amount_inr'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 9\n",
    "\n",
    "def correct_prices(group):\n",
    "    median_price = group['original_price_inr_cleaned'].median()\n",
    "    threshold = 3 * median_price\n",
    "    \n",
    "    # Create new column 'corrected_price' starting as copy of original 'price'\n",
    "    group['corrected_price'] = group['original_price_inr_cleaned']\n",
    "    \n",
    "    outliers = group['original_price_inr_cleaned'] > threshold\n",
    "    correction_candidates = (group['original_price_inr_cleaned'] / 100) < threshold\n",
    "    \n",
    "    # Only update 'corrected_price' column; original 'price' unchanged\n",
    "    group.loc[outliers & correction_candidates, 'corrected_price'] = group.loc[outliers & correction_candidates, 'original_price_inr_cleaned'] / 100\n",
    "    \n",
    "    return group\n",
    "\n",
    "# df = df.groupby('product_id').apply(correct_prices).reset_index(drop=True)\n",
    "\n",
    "# Apply function per product\n",
    "df = df.groupby('product_id').apply(correct_prices).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 10\n",
    "\n",
    "def standardize_payment_method(payment_method: str) -> str:\n",
    "    payment_method = str(payment_method).lower()  # Convert to string and lowercase\n",
    "\n",
    "    payment_map = {\n",
    "        'UPI': ['upi', 'phonepe', 'googlepay'],\n",
    "        'Credit Card': ['credit card', 'credit_card', 'cc'],\n",
    "        'Debit Card': ['debit card', 'debit_card', 'dc'],\n",
    "        'Cash on Delivery': ['cash on delivery', 'cod', 'c.o.d'],\n",
    "        # Add more mappings if needed\n",
    "    }\n",
    "\n",
    "    for category, keywords in payment_map.items():\n",
    "        if any(keyword in payment_method for keyword in keywords):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "# Apply the function to the payment_method column\n",
    "df['standard_payment_method'] = df['payment_method'].apply(standardize_payment_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "class DataCleaning():\n",
    "    def __init__(self):\n",
    "        self.city_names_corrections = {\n",
    "            \"chenai\": \"chennai\",\n",
    "            \"delhi ncr\": \"delhi\",\n",
    "            \"navi mumbai\": \"mumbai\",\n",
    "        }\n",
    "        self.payment_map = {\n",
    "            'UPI': ['upi', 'phonepe', 'googlepay'],\n",
    "            'Credit Card': ['credit card', 'credit_card', 'cc'],\n",
    "            'Debit Card': ['debit card', 'debit_card', 'dc'],\n",
    "            'Cash on Delivery': ['cash on delivery', 'cod', 'c.o.d'],\n",
    "            \"Net Banking\": ['net banking', 'net_banking', 'nb'],\n",
    "            \"Wallet\": ['wallet'],\n",
    "            \"Buy Now Pay Later\": ['buy now pay later', 'bnpl'],\n",
    "            # Add more mappings if needed\n",
    "        }\n",
    "        self.max_delivery_days = 15  # Set a maximum threshold for delivery days\n",
    "\n",
    "    # Question 1: Date Cleaning\n",
    "    def date_cleaning(self, date_str):\n",
    "        try:\n",
    "            # Use dateutil parser with dayfirst=True for mixed formats\n",
    "            date_time = parser.parse(date_str, dayfirst=True)\n",
    "            return date_time.date()\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    # Question 2: Price Cleaning\n",
    "    def price_cleaning(self, value):\n",
    "        if isinstance(value, (int, float)):  # Already numeric\n",
    "            return value\n",
    "        if isinstance(value, str):\n",
    "            # Remove currency symbols and commas, keep digits and dot\n",
    "            cleaned = re.sub(r'[^\\d.]', '', value)\n",
    "            try:\n",
    "                return float(cleaned) if '.' in cleaned else int(cleaned)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan # For other types (e.g., None)\n",
    "    \n",
    "    # Question 3: Rating Standardization\n",
    "    def round_up_scale(self, value):\n",
    "        \"\"\"Round value up to nearest 5 or 10, whichever is appropriate.\"\"\"\n",
    "        if value <= 5:\n",
    "            return 5\n",
    "        elif value <= 10:\n",
    "            return 10\n",
    "        else:\n",
    "            # For values above 10, round up to nearest multiple of 5\n",
    "            return int(math.ceil(value / 5.0)) * 5\n",
    "        \n",
    "    def standardize_rating(self, rating):\n",
    "        if rating is None:\n",
    "            return np.nan\n",
    "\n",
    "        rating_str = str(rating).strip().lower()\n",
    "\n",
    "        # Fraction parsing\n",
    "        if '/' in rating_str:\n",
    "            try:\n",
    "                numerator, denominator = rating_str.split('/')\n",
    "                numerator = float(numerator)\n",
    "                denominator = float(denominator)\n",
    "                if denominator == 0:\n",
    "                    return np.nan\n",
    "                normalized = (numerator / denominator) * 5\n",
    "                if 1.0 <= normalized <= 5.0:\n",
    "                    return round(normalized, 2)\n",
    "                else:\n",
    "                    return np.nan\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "        # Extract numeric value\n",
    "        match = re.search(r'(\\d+(\\.\\d+)?)', rating_str)\n",
    "        if match:\n",
    "            try:\n",
    "                value = float(match.group(1))\n",
    "\n",
    "                scale = self.round_up_scale(value)\n",
    "                normalized = (value / scale) * 5\n",
    "\n",
    "                # Clamp between 1 and 5\n",
    "                normalized = max(1.0, min(normalized, 5.0))\n",
    "                return round(normalized, 2)\n",
    "\n",
    "            except:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "    \n",
    "    # Question 4: City Name Standardization\n",
    "    def geocode_city_osm(self, city, geolocator):\n",
    "        try:\n",
    "            # Limit search to India for better accuracy\n",
    "            location = geolocator.geocode(f\"{city}, India\", language='en', exactly_one=True, addressdetails=True)\n",
    "            if location and 'address' in location.raw:\n",
    "                address = location.raw['address']\n",
    "                # Extract city or town from the address details\n",
    "                city_name = address.get('city') or address.get('town') or address.get('village')\n",
    "                \n",
    "                # Sometimes the city might be under 'state_district' or 'county' if city not found\n",
    "                if not city_name:\n",
    "                    city_name = address.get('state_district') or address.get('county')\n",
    "                \n",
    "                if city_name:\n",
    "                    return city_name\n",
    "                else:\n",
    "                    # Fallback: use first part of display_name\n",
    "                    return location.address.split(',')[0]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding '{city}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    def city_standardization(self, unique_cities):\n",
    "        geolocator = Nominatim(user_agent=\"osm_city_standardizer\")\n",
    "\n",
    "        # Step 1: Get unique city names\n",
    "        for i in range(len(unique_cities)):\n",
    "            if unique_cities[i].lower() in self.city_names_corrections:\n",
    "                unique_cities[i] = self.city_names_corrections[unique_cities[i].lower()]\n",
    "\n",
    "        # Step 2: Build a mapping dictionary\n",
    "        city_mapping = {}\n",
    "\n",
    "        for city in unique_cities:\n",
    "            print(f\"input city is: {city}\")\n",
    "            standardized_city = self.geocode_city_osm(city=city, geolocator=geolocator)\n",
    "            print(f\"output city is: {standardized_city}\")\n",
    "            city_mapping[city] = standardized_city\n",
    "            time.sleep(1)  # Pause to respect Nominatim's usage policy (1 sec per request)\n",
    "        \n",
    "        for key, value in city_mapping.items():\n",
    "            city_mapping[key] = self.city_names_corrections.get(value.lower(), value).title()\n",
    "            print(f\"after correction city is: {key, city_mapping[key]}\")\n",
    "        return city_mapping\n",
    "    \n",
    "    # Question 5: Boolean Normalization\n",
    "    def normalize_boolean(self, val):\n",
    "        if pd.isna(val):\n",
    "            return False  # or pd.NA if you want to keep missing values\n",
    "        if isinstance(val, str):\n",
    "            val = val.strip().lower()\n",
    "            if val in ['yes', 'y', 'true', '1']:\n",
    "                return True\n",
    "            elif val in ['no', 'n', 'false', '0']:\n",
    "                return False\n",
    "            else:\n",
    "                return pd.NA  # unrecognized string\n",
    "        return bool(val)\n",
    "    \n",
    "    # Question 6: Category Standardization\n",
    "    def standardize_category(self, categories):\n",
    "        output = {}\n",
    "        for i in categories:\n",
    "            output[i] = \"Electronics\" if \"electronic\" in i.lower() else i\n",
    "        return output\n",
    "    \n",
    "    # Question 7: Delivery Days Cleaning    \n",
    "    def clean_delivery_day(self, value, max_valid_days=None):\n",
    "        \"\"\"\n",
    "        Cleans delivery day input and returns a numeric value representing delivery days.\n",
    "        Returns np.nan for invalid or unrealistic values.\n",
    "        \"\"\"\n",
    "        if not max_valid_days:\n",
    "            max_valid_days = self.max_delivery_days\n",
    "        if not isinstance(value, str):\n",
    "            return np.nan\n",
    "\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Special case: 'same day'\n",
    "        if value == \"same day\":\n",
    "            return 0\n",
    "\n",
    "        # Extract all numeric parts (integers or decimals)\n",
    "        range_match = re.findall(r\"\\d+(?:\\.\\d+)?\", value)\n",
    "\n",
    "        if '-' in value or 'to' in value:\n",
    "            # Handle range like \"1-2 days\" or \"1 to 3 business days\"\n",
    "            if len(range_match) == 2:\n",
    "                try:\n",
    "                    high = max(float(range_match[0]), float(range_match[1]))\n",
    "                    return high if high <= max_valid_days else np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "            else:\n",
    "                return np.nan\n",
    "\n",
    "        # Single number case (e.g., '2 days', '3 business days')\n",
    "        if len(range_match) == 1:\n",
    "            try:\n",
    "                num = float(range_match[0])\n",
    "                if num < 0 or num > max_valid_days:\n",
    "                    return np.nan\n",
    "                return num\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "        # Unrecognized format\n",
    "        return np.nan\n",
    "    \n",
    "    # Question 8: Duplicate Transaction Identification\n",
    "    def analyze_group(self, group):\n",
    "        unique_tx_ids = group['transaction_id'].nunique()\n",
    "        total_records = len(group)\n",
    "        \n",
    "        # All unique transaction IDs\n",
    "        if unique_tx_ids == total_records:\n",
    "            return 'genuine_bulk_order'\n",
    "        \n",
    "        # Repeated transaction IDs with identical other features\n",
    "        tx_id_counts = group['transaction_id'].value_counts()\n",
    "        repeated_tx_ids = tx_id_counts[tx_id_counts > 1].index\n",
    "\n",
    "        for tx_id in repeated_tx_ids:\n",
    "            repeated_tx_group = group[group['transaction_id'] == tx_id]\n",
    "            # Check if all rows for this transaction ID are exactly the same\n",
    "            if repeated_tx_group.drop(columns='transaction_id').duplicated().all():\n",
    "                return 'data_error_duplicates'\n",
    "        \n",
    "        # Could be mix of genuine and error\n",
    "        return 'ambiguous'\n",
    "    \n",
    "    # Question 9: Price Correction\n",
    "    def mad_based_threshold(self, group, column_name, threshold_multiplier=3):\n",
    "        median_price = group[column_name].median()\n",
    "        mad = (group[column_name] - median_price).abs().median()\n",
    "        threshold = median_price + threshold_multiplier * mad\n",
    "        return threshold\n",
    "\n",
    "\n",
    "    def correct_prices(self, group, threshold_multiplier=3):\n",
    "        median_price = group['clean_original_price_inr'].median()\n",
    "        # threshold = threshold_multiplier * median_price\n",
    "        threshold = self.mad_based_threshold(group, column_name='clean_original_price_inr', threshold_multiplier=threshold_multiplier)\n",
    "        \n",
    "        # Start corrected_price as a copy to avoid SettingWithCopyWarning\n",
    "        group = group.copy()\n",
    "        group['corrected_price'] = group['clean_original_price_inr']\n",
    "        \n",
    "        # Identify outliers beyond threshold\n",
    "        outliers = group['clean_original_price_inr'] > threshold\n",
    "        \n",
    "        # Check if dividing by 100 brings price below threshold\n",
    "        correction_candidates = (group['clean_original_price_inr'] / 100) < threshold\n",
    "        \n",
    "        # Apply correction where both conditions hold\n",
    "        condition = outliers & correction_candidates\n",
    "        group.loc[condition, 'corrected_price'] = group.loc[condition, 'clean_original_price_inr'] / 100\n",
    "\n",
    "        # Optional: print or log number of corrections made\n",
    "        num_corrected = condition.sum()\n",
    "        # if num_corrected > 0:\n",
    "        #     print(f\"Corrected {num_corrected} prices in product_id {group.name}\")\n",
    "        \n",
    "        return group\n",
    "    \n",
    "    # Question 10: Payment Method Standardization\n",
    "    def standardize_payment_method(self, payment_method: str) -> str:\n",
    "        payment_method = str(payment_method).lower()  # Convert to string and lowercase\n",
    "\n",
    "        for category, keywords in self.payment_map.items():\n",
    "            if any(keyword in payment_method for keyword in keywords):\n",
    "                return category\n",
    "        return 'Other'\n",
    "\n",
    "# [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
    "years = [2025]\n",
    "for i in years:\n",
    "    input_path = fr\"C:\\Users\\haris\\OneDrive\\Desktop\\Guvi\\Projects\\Amazon_Sales_Analysis\\dataset\\amazon_india_{i}.csv\"\n",
    "    output_path = fr\"C:\\Users\\haris\\OneDrive\\Desktop\\Guvi\\Projects\\Amazon_Sales_Analysis\\dataset\\amazon_india_{i}_cleaned.csv\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    data_cleaner = DataCleaning()\n",
    "\n",
    "    # Question 1\n",
    "    df[\"clean_order_date\"] = df[\"order_date\"].apply(data_cleaner.date_cleaning)\n",
    "\n",
    "    # Question 2\n",
    "    columns = [\"original_price_inr\", \"discount_percent\", \"final_amount_inr\", \"delivery_charges\"]\n",
    "    df[[\"clean_original_price_inr\", \"clean_discount_percent\", \"clean_final_amount_inr\", \"clean_delivery_charges\"]] = df[columns].map(data_cleaner.price_cleaning)\n",
    "\n",
    "    # Question 3\n",
    "    columns = ['customer_rating', 'product_rating']\n",
    "    df[[\"cleaned_customer_rating\", \"cleaned_product_rating\"]] = df[columns].map(data_cleaner.standardize_rating)\n",
    "\n",
    "    # Question 4\n",
    "    cleaned_citys = data_cleaner.city_standardization(df['customer_city'].astype(str).str.lower().unique())\n",
    "    df[\"cleaned_customer_city\"] = df[\"customer_city\"].astype(str).str.lower().map(cleaned_citys)\n",
    "\n",
    "    # Question 5\n",
    "    columns = ['is_prime_member', 'is_prime_eligible', 'is_festival_sale']\n",
    "    df[['cleaned_is_prime_member', 'cleaned_is_prime_eligible', 'cleaned_is_festival_sale']] = df[columns].map(data_cleaner.normalize_boolean)\n",
    "\n",
    "    # Question 6\n",
    "    category_map = data_cleaner.standardize_category(df[\"category\"].astype(str).unique())\n",
    "    df[\"cleaned_category\"] = df[\"category\"].map(category_map)\n",
    "\n",
    "    # Question 7\n",
    "    df[\"cleaned_delivery_days\"] = df[\"delivery_days\"].apply(data_cleaner.clean_delivery_day)\n",
    "\n",
    "    # Question 8\n",
    "    grouped = df.groupby(['customer_id', 'product_id', 'order_date', 'final_amount_inr'])\n",
    "    # Apply the function to each group and create a Series with the result repeated for each row in the group\n",
    "    duplicate_types = grouped.apply(data_cleaner.analyze_group, include_groups=False).reset_index(name='duplicate_type')\n",
    "\n",
    "    # Map the group keys back to the original dataframe by merging or joining\n",
    "    df = df.merge(duplicate_types.reset_index(), on=['customer_id', 'product_id', 'order_date', 'final_amount_inr'], how='left')\n",
    "    if 'index' in df.columns:\n",
    "        df.drop(columns='index', inplace=True)\n",
    "\n",
    "    # Question 9\n",
    "    df = df.groupby('product_id').apply(data_cleaner.correct_prices).reset_index(drop=True)\n",
    "\n",
    "    # Question 10\n",
    "    df['standard_payment_method'] = df['payment_method'].apply(data_cleaner.standardize_payment_method)\n",
    "\n",
    "    # save the output to a csv file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"cleaned data saved to {output_path}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "458f04e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transaction_id', 'order_date', 'customer_id', 'product_id',\n",
       "       'product_name', 'category', 'subcategory', 'brand',\n",
       "       'original_price_inr', 'discount_percent', 'discounted_price_inr',\n",
       "       'quantity', 'subtotal_inr', 'delivery_charges', 'final_amount_inr',\n",
       "       'customer_city', 'customer_state', 'customer_tier',\n",
       "       'customer_spending_tier', 'customer_age_group', 'payment_method',\n",
       "       'delivery_days', 'delivery_type', 'is_prime_member', 'is_festival_sale',\n",
       "       'festival_name', 'customer_rating', 'return_status', 'order_month',\n",
       "       'order_year', 'order_quarter', 'product_weight_kg', 'is_prime_eligible',\n",
       "       'product_rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6589d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\haris\\OneDrive\\Desktop\\Guvi\\Projects\\Amazon_Sales_Analysis\\dataset\\amazon_india_2017_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fe82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"order_date\", \"clean_order_date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"original_price_inr\", \"clean_original_price_inr\", \"discount_percent\", \"clean_discount_percent\", \"final_amount_inr\", \"clean_final_amount_inr\", \"delivery_charges\", \"clean_delivery_charges\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"product_rating\", \"cleaned_product_rating\", \"customer_rating\", \"cleaned_customer_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"is_prime_member\", \"cleaned_is_prime_member\", \"is_prime_eligible\", \"cleaned_is_prime_eligible\", \"is_festival_sale\", \"cleaned_is_festival_sale\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4a6eb514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['False', False, 'True', True, 'FALSE', '0', 'No', 'Yes', 'TRUE',\n",
       "       '1'], dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(df[[\"is_prime_member\", \"cleaned_is_prime_member\", \"is_prime_eligible\", \"cleaned_is_prime_eligible\", \"is_festival_sale\", \"cleaned_is_festival_sale\"]].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "34b2b0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(df[[\"cleaned_is_prime_member\", \"cleaned_is_prime_eligible\", \"cleaned_is_festival_sale\"]].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ba87634d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Electronics', 'ELECTRONICS', 'Electronicss',\n",
       "       'Electronics & Accessories', 'Electronic'], dtype=object)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e72a5803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55270</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55271</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55272</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55273</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55274</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          category cleaned_category\n",
       "0      Electronics      Electronics\n",
       "1      Electronics      Electronics\n",
       "2      Electronics      Electronics\n",
       "3      Electronics      Electronics\n",
       "4      Electronics      Electronics\n",
       "...            ...              ...\n",
       "55270  Electronics      Electronics\n",
       "55271  Electronics      Electronics\n",
       "55272  Electronics      Electronics\n",
       "55273  Electronics      Electronics\n",
       "55274  Electronics      Electronics\n",
       "\n",
       "[55275 rows x 2 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"category\", \"cleaned_category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b34b8ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['genuine_bulk_order'], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"duplicate_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d67b8078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UPI', 'Credit Card', 'BNPL', 'COD', 'Debit Card', 'Net Banking',\n",
       "       'Wallet'], dtype=object)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"payment_method\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "cf17b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\haris\\OneDrive\\Desktop\\Guvi\\Projects\\Amazon_Sales_Analysis\\dataset\\cleaned_dataset\\amazon_india_2025_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ddbc8abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transaction_id', 'order_date', 'customer_id', 'product_id',\n",
       "       'product_name', 'category', 'subcategory', 'brand',\n",
       "       'original_price_inr', 'discount_percent', 'discounted_price_inr',\n",
       "       'quantity', 'subtotal_inr', 'delivery_charges', 'final_amount_inr',\n",
       "       'customer_city', 'customer_state', 'customer_tier',\n",
       "       'customer_spending_tier', 'customer_age_group', 'payment_method',\n",
       "       'delivery_days', 'delivery_type', 'is_prime_member', 'is_festival_sale',\n",
       "       'festival_name', 'customer_rating', 'return_status', 'order_month',\n",
       "       'order_year', 'order_quarter', 'product_weight_kg', 'is_prime_eligible',\n",
       "       'product_rating', 'clean_order_date', 'clean_original_price_inr',\n",
       "       'clean_discount_percent', 'clean_final_amount_inr',\n",
       "       'clean_delivery_charges', 'cleaned_customer_rating',\n",
       "       'cleaned_product_rating', 'cleaned_customer_city',\n",
       "       'cleaned_is_prime_member', 'cleaned_is_prime_eligible',\n",
       "       'cleaned_is_festival_sale', 'cleaned_category', 'cleaned_delivery_days',\n",
       "       'duplicate_type', 'corrected_price', 'standard_payment_method'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf83b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
